{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOSE playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Arguments Settting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Arguments =\n",
      "\tseed: 0\n",
      "\tlr: 0.001\n",
      "\twd: 0.0001\n",
      "\tdataset: cifar100\n",
      "\tbuffer_size: 5000\n",
      "\tbuffer_batch_size: 64\n",
      "\trun_nums: 1\n",
      "\tbatch_size: 10\n",
      "\tins_t: 0.07\n",
      "\tgpu_id: 5\n",
      "\tn_workers: 8\n",
      "\tcuda: True\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import warnings\n",
    "from multi_runs import multiple_run\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "from experiment.dataset import get_data\n",
    "from models.buffer import Buffer\n",
    "from train_scd import TrainLearner_SCD\n",
    "from models.Resnet18_SD import resnet18_sd\n",
    "from utils.util import compute_performance\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "args = argparse.Namespace()\n",
    "setattr(args, 'seed', 0)\n",
    "setattr(args, 'lr', 1e-3)\n",
    "setattr(args, 'wd', 1e-4)\n",
    "setattr(args, 'dataset', 'cifar100')\n",
    "setattr(args, 'buffer_size', 5000)\n",
    "setattr(args, 'buffer_batch_size', 64)\n",
    "setattr(args, 'run_nums', 1)\n",
    "setattr(args, 'batch_size', 10)\n",
    "setattr(args, 'ins_t', 0.07)\n",
    "setattr(args, 'gpu_id', 5)\n",
    "setattr(args, 'n_workers', 8)\n",
    "\n",
    "torch.cuda.set_device(args.gpu_id)\n",
    "args.cuda = torch.cuda.is_available()\n",
    "print('=' * 100)\n",
    "print('Arguments =')\n",
    "for arg in vars(args):\n",
    "    print('\\t' + arg + ':', getattr(args, arg))\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    print('[CUDA is unavailable]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all_acc = torch.zeros(args.run_nums)\n",
    "accuracy_list = []\n",
    "\n",
    "for run in range(args.run_nums):\n",
    "    tmp_acc = []\n",
    "    print('=' * 100)\n",
    "    print(f\"-----------------------------run {run} start--------------------------\")\n",
    "    print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    print('=' * 100)\n",
    "\n",
    "    # initialize\n",
    "    data, class_num, class_per_task, task_loader, input_size = get_data(args.dataset, args.batch_size, args.n_workers)\n",
    "    args.n_classes = class_num\n",
    "    buffer = Buffer(args, input_size).cuda()\n",
    "\n",
    "    model = resnet18_sd(class_num).cuda()\n",
    "    optimizer = Adam(model.parameters(), lr=args.lr,  weight_decay=1e-4)\n",
    "    agent = TrainLearner_SCD(model, buffer, optimizer, class_num, class_per_task, input_size, args)\n",
    "\n",
    "    # training\n",
    "    for i in range(len(task_loader)):\n",
    "        print(f\"-----------------------------run {run} task id:{i} start training-----------------------------\")\n",
    "\n",
    "        agent.train(i, task_loader[i]['train'])\n",
    "        acc_list = agent.test(i, task_loader)\n",
    "        tmp_acc.append(acc_list)\n",
    "\n",
    "    test_accuracy = acc_list.mean()\n",
    "    test_all_acc[run] = test_accuracy\n",
    "    accuracy_list.append(np.array(tmp_acc))\n",
    "\n",
    "    print('=' * 100)\n",
    "    print(\"{}th run's Test result: Accuracy: {:.2f}%\".format(run, test_accuracy))\n",
    "    print('=' * 100)\n",
    "\n",
    "    agent.save_checkpoint('./outputs/final.pt')\n",
    "\n",
    "accuracy_array = np.array(accuracy_list)\n",
    "avg_end_acc, avg_end_fgt, avg_acc, avg_bwtp, avg_fwt = compute_performance(accuracy_array)\n",
    "print('=' * 100)\n",
    "print(f\"total {args.run_nums}runs test acc results: {test_all_acc}\")\n",
    "print('----------- Avg_End_Acc {} Avg_End_Fgt {} Avg_Acc {} Avg_Bwtp {} Avg_Fwt {}-----------'\n",
    "        .format(avg_end_acc, avg_end_fgt, avg_acc, avg_bwtp, avg_fwt))\n",
    "print('=' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Ensemble Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task order = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Data and loader is prepared\n",
      "buffer has 5000 slots\n",
      "bx torch.Size([5000, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "data, class_num, class_per_task, task_loader, input_size = get_data(args.dataset, args.batch_size, args.n_workers)\n",
    "args.n_classes = class_num\n",
    "buffer = Buffer(args, input_size).cuda()\n",
    "model = resnet18_sd(class_num).cuda()\n",
    "optimizer = Adam(model.parameters(), lr=args.lr,  weight_decay=1e-4)\n",
    "agent = TrainLearner_SCD(model, buffer, optimizer, class_num, class_per_task, input_size, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoint from: ./outputs/final.pt\n"
     ]
    }
   ],
   "source": [
    "# loading\n",
    "agent.load_checkpoint('./outputs/final.pt')\n",
    "agent.buffer.current_index = args.buffer_size - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent, i, task_loader, feat_ids=[3]):\n",
    "    agent.model.eval()\n",
    "    acc_lists = []\n",
    "    for feat_id in feat_ids:\n",
    "        print(f\"{'*'*100}\\nTest with the output of layer: {feat_id+1}\\n\")\n",
    "        agent.class_means = {}\n",
    "        agent.seen_classes = list(set(agent.buffer.y_int.tolist()))\n",
    "        class_inputs = {cls: [] for cls in agent.seen_classes}\n",
    "        for x, y in zip(agent.buffer.x, agent.buffer.y_int):\n",
    "            class_inputs[y.item()].append(x)\n",
    "\n",
    "        for cls, inputs in class_inputs.items():\n",
    "            features = []\n",
    "            for ex in inputs:\n",
    "                # feature = agent.model.final_feature(ex.unsqueeze(0)).detach().clone()\n",
    "                feature = agent.model.features(ex.unsqueeze(0))[feat_id].detach().clone()\n",
    "                feature = F.normalize(feature, dim=1)\n",
    "                features.append(feature.squeeze())\n",
    "\n",
    "            if len(features) == 0:\n",
    "                mu_y = torch.normal(\n",
    "                    # 0, 1, size=tuple(agent.model.final_feature(x.unsqueeze(0)).detach().size())\n",
    "                    0, 1, size=tuple(agent.model.features(x.unsqueeze(0))[feat_id].detach().size())\n",
    "                )\n",
    "                mu_y = mu_y.to(x.device)\n",
    "            else:\n",
    "                features = torch.stack(features)\n",
    "                mu_y = features.mean(0)\n",
    "            mu_y = F.normalize(mu_y.reshape(1, -1), dim=1)\n",
    "            agent.class_means[cls] = mu_y.squeeze()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            acc_list = np.zeros(len(task_loader))\n",
    "            for j in range(i + 1):\n",
    "                acc = agent.test_model(task_loader[j]['test'], j, feat_id=feat_id)\n",
    "                acc_list[j] = acc.item()\n",
    "\n",
    "            print(f\"tasks acc:{acc_list}\")\n",
    "            print(f\"tasks avg acc:{acc_list[:i+1].mean()}\")\n",
    "        acc_lists.append(acc_list)\n",
    "\n",
    "    return acc_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Test with the output of layer: 1\n",
      "\n",
      "Test task 0: Accuracy: 509/1000 (50.90%)\n",
      "Test task 1: Accuracy: 446/1000 (44.60%)\n",
      "Test task 2: Accuracy: 504/1000 (50.40%)\n",
      "Test task 3: Accuracy: 417/1000 (41.70%)\n",
      "Test task 4: Accuracy: 481/1000 (48.10%)\n",
      "Test task 5: Accuracy: 525/1000 (52.50%)\n",
      "Test task 6: Accuracy: 473/1000 (47.30%)\n",
      "Test task 7: Accuracy: 484/1000 (48.40%)\n",
      "Test task 8: Accuracy: 503/1000 (50.30%)\n",
      "Test task 9: Accuracy: 495/1000 (49.50%)\n",
      "tasks acc:[50.90000153 44.59999847 50.40000153 41.70000076 48.09999847 52.5\n",
      " 47.29999924 48.40000153 50.29999924 49.5       ]\n",
      "tasks avg acc:48.37000007629395\n",
      "****************************************************************************************************\n",
      "Test with the output of layer: 2\n",
      "\n",
      "Test task 0: Accuracy: 487/1000 (48.70%)\n",
      "Test task 1: Accuracy: 477/1000 (47.70%)\n",
      "Test task 2: Accuracy: 552/1000 (55.20%)\n",
      "Test task 3: Accuracy: 434/1000 (43.40%)\n",
      "Test task 4: Accuracy: 518/1000 (51.80%)\n",
      "Test task 5: Accuracy: 532/1000 (53.20%)\n",
      "Test task 6: Accuracy: 486/1000 (48.60%)\n",
      "Test task 7: Accuracy: 485/1000 (48.50%)\n",
      "Test task 8: Accuracy: 521/1000 (52.10%)\n",
      "Test task 9: Accuracy: 531/1000 (53.10%)\n",
      "tasks acc:[48.70000076 47.70000076 55.20000076 43.40000153 51.79999924 53.20000076\n",
      " 48.59999847 48.5        52.09999847 53.09999847]\n",
      "tasks avg acc:50.22999992370605\n",
      "****************************************************************************************************\n",
      "Test with the output of layer: 3\n",
      "\n",
      "Test task 0: Accuracy: 495/1000 (49.50%)\n",
      "Test task 1: Accuracy: 454/1000 (45.40%)\n",
      "Test task 2: Accuracy: 528/1000 (52.80%)\n",
      "Test task 3: Accuracy: 428/1000 (42.80%)\n",
      "Test task 4: Accuracy: 496/1000 (49.60%)\n",
      "Test task 5: Accuracy: 543/1000 (54.30%)\n",
      "Test task 6: Accuracy: 497/1000 (49.70%)\n",
      "Test task 7: Accuracy: 528/1000 (52.80%)\n",
      "Test task 8: Accuracy: 570/1000 (57.00%)\n",
      "Test task 9: Accuracy: 598/1000 (59.80%)\n",
      "tasks acc:[49.5        45.40000153 52.79999924 42.79999924 49.59999847 54.29999924\n",
      " 49.70000076 52.79999924 57.         59.79999924]\n",
      "tasks avg acc:51.369999694824216\n",
      "****************************************************************************************************\n",
      "Test with the output of layer: 4\n",
      "\n",
      "Test task 0: Accuracy: 485/1000 (48.50%)\n",
      "Test task 1: Accuracy: 432/1000 (43.20%)\n",
      "Test task 2: Accuracy: 526/1000 (52.60%)\n",
      "Test task 3: Accuracy: 402/1000 (40.20%)\n",
      "Test task 4: Accuracy: 457/1000 (45.70%)\n",
      "Test task 5: Accuracy: 535/1000 (53.50%)\n",
      "Test task 6: Accuracy: 458/1000 (45.80%)\n",
      "Test task 7: Accuracy: 506/1000 (50.60%)\n",
      "Test task 8: Accuracy: 564/1000 (56.40%)\n",
      "Test task 9: Accuracy: 627/1000 (62.70%)\n",
      "tasks acc:[48.5        43.20000076 52.59999847 40.20000076 45.70000076 53.5\n",
      " 45.79999924 50.59999847 56.40000153 62.70000076]\n",
      "tasks avg acc:49.920000076293945\n"
     ]
    }
   ],
   "source": [
    "# get results of all layers\n",
    "acc_lists = test(agent, 9, task_loader, feat_ids=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./outputs/acc_lists.txt', 'w') as f:\n",
    "    for acc_list in acc_lists:\n",
    "        for acc in acc_list:\n",
    "            f.write(f\"{acc:.2f}\\t\")\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 10)\n",
      "[50.90000153 47.70000076 55.20000076 43.40000153 51.79999924 54.29999924\n",
      " 49.70000076 52.79999924 57.         62.70000076]\n",
      "52.55000038146973\n"
     ]
    }
   ],
   "source": [
    "acc_lists = np.array(acc_lists)\n",
    "print(acc_lists.shape)\n",
    "opt_acc_list = acc_lists.max(axis=0)\n",
    "print(opt_acc_list)\n",
    "print(opt_acc_list.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.model.eval()\n",
    "class_means_ls = [{} for _ in range(4)]\n",
    "seen_classes = list(set(agent.buffer.y_int.tolist()))\n",
    "class_inputs = {cls: [] for cls in seen_classes}\n",
    "\n",
    "for x, y in zip(agent.buffer.x, agent.buffer.y_int):\n",
    "    class_inputs[y.item()].append(x)\n",
    "\n",
    "for cls, inputs in class_inputs.items():\n",
    "    features = [[] for _ in range(4)]\n",
    "    for ex in inputs:\n",
    "        features_ls = agent.model.features(ex.unsqueeze(0))\n",
    "        for feat_id in range(4):\n",
    "            feature = features_ls[feat_id].detach().clone()\n",
    "            feature = F.normalize(feature, dim=1)\n",
    "            features[feat_id].append(feature.squeeze())\n",
    "\n",
    "    for feat_id in range(4):\n",
    "        if len(features[feat_id]) == 0:\n",
    "            mu_y = torch.normal(\n",
    "                0, 1, size=tuple(agent.model.features(x.unsqueeze(0))[feat_id].detach().size())\n",
    "            )\n",
    "            mu_y = mu_y.to(x.device)\n",
    "        else:\n",
    "            features[feat_id] = torch.stack(features[feat_id])\n",
    "            mu_y = features[feat_id].mean(0)\n",
    "        \n",
    "        mu_y = F.normalize(mu_y.reshape(1, -1), dim=1)\n",
    "        class_means_ls[feat_id][cls] = mu_y.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(class_means_ls, './outputs/class_means_ls.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_means_ls = torch.load('./outputs/class_means_ls.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test with feature layer 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test task 0: Accuracy: 509/1000 (50.90%)\n",
      "Test task 1: Accuracy: 446/1000 (44.60%)\n",
      "Test task 2: Accuracy: 504/1000 (50.40%)\n",
      "Test task 3: Accuracy: 417/1000 (41.70%)\n",
      "Test task 4: Accuracy: 481/1000 (48.10%)\n",
      "Test task 5: Accuracy: 525/1000 (52.50%)\n",
      "Test task 6: Accuracy: 473/1000 (47.30%)\n",
      "Test task 7: Accuracy: 484/1000 (48.40%)\n",
      "Test task 8: Accuracy: 503/1000 (50.30%)\n",
      "Test task 9: Accuracy: 495/1000 (49.50%)\n",
      "Average End Accuracy: 48.37%\n",
      "\n",
      "Test with feature layer 1\n",
      "Test task 0: Accuracy: 487/1000 (48.70%)\n",
      "Test task 1: Accuracy: 477/1000 (47.70%)\n",
      "Test task 2: Accuracy: 552/1000 (55.20%)\n",
      "Test task 3: Accuracy: 434/1000 (43.40%)\n",
      "Test task 4: Accuracy: 518/1000 (51.80%)\n",
      "Test task 5: Accuracy: 532/1000 (53.20%)\n",
      "Test task 6: Accuracy: 486/1000 (48.60%)\n",
      "Test task 7: Accuracy: 485/1000 (48.50%)\n",
      "Test task 8: Accuracy: 521/1000 (52.10%)\n",
      "Test task 9: Accuracy: 531/1000 (53.10%)\n",
      "Average End Accuracy: 50.23%\n",
      "\n",
      "Test with feature layer 2\n",
      "Test task 0: Accuracy: 495/1000 (49.50%)\n",
      "Test task 1: Accuracy: 454/1000 (45.40%)\n",
      "Test task 2: Accuracy: 528/1000 (52.80%)\n",
      "Test task 3: Accuracy: 428/1000 (42.80%)\n",
      "Test task 4: Accuracy: 496/1000 (49.60%)\n",
      "Test task 5: Accuracy: 543/1000 (54.30%)\n",
      "Test task 6: Accuracy: 497/1000 (49.70%)\n",
      "Test task 7: Accuracy: 528/1000 (52.80%)\n",
      "Test task 8: Accuracy: 570/1000 (57.00%)\n",
      "Test task 9: Accuracy: 598/1000 (59.80%)\n",
      "Average End Accuracy: 51.37%\n",
      "\n",
      "Test with feature layer 3\n",
      "Test task 0: Accuracy: 485/1000 (48.50%)\n",
      "Test task 1: Accuracy: 432/1000 (43.20%)\n",
      "Test task 2: Accuracy: 526/1000 (52.60%)\n",
      "Test task 3: Accuracy: 402/1000 (40.20%)\n",
      "Test task 4: Accuracy: 457/1000 (45.70%)\n",
      "Test task 5: Accuracy: 535/1000 (53.50%)\n",
      "Test task 6: Accuracy: 458/1000 (45.80%)\n",
      "Test task 7: Accuracy: 506/1000 (50.60%)\n",
      "Test task 8: Accuracy: 564/1000 (56.40%)\n",
      "Test task 9: Accuracy: 627/1000 (62.70%)\n",
      "Average End Accuracy: 49.92%\n"
     ]
    }
   ],
   "source": [
    "# test ncm classifier\n",
    "seen_classes = list(set(agent.buffer.y_int.tolist()))\n",
    "agent.model.eval()\n",
    "for feat_id in range(4):\n",
    "    print(f\"\\nTest with feature layer {feat_id}\")\n",
    "    class_means = class_means_ls[feat_id]\n",
    "    with torch.no_grad():\n",
    "        acc_list = np.zeros(len(task_loader))\n",
    "        for j in range(10):\n",
    "            correct = torch.full([], 0).cuda()\n",
    "            num = torch.full([], 0).cuda()\n",
    "            for batch_idx, (data, target) in enumerate(task_loader[j]['test']):\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                features = agent.model.features(data)[feat_id]\n",
    "                features = F.normalize(features, dim=1)\n",
    "                features = features.unsqueeze(2)\n",
    "                means = torch.stack([class_means[cls] for cls in seen_classes])\n",
    "                means = torch.stack([means] * data.size(0))\n",
    "                means = means.transpose(1, 2)\n",
    "                features = features.expand_as(means)\n",
    "                dists = (features - means).pow(2).sum(1).squeeze()\n",
    "                pred = dists.min(1)[1]\n",
    "                pred = torch.Tensor(seen_classes)[pred].to(data.device)\n",
    "\n",
    "                num += data.size()[0]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "            test_accuracy = (100. * correct / num)\n",
    "            acc_list[j] = test_accuracy\n",
    "            print('Test task {}: Accuracy: {}/{} ({:.2f}%)'.format(j, correct, num, test_accuracy))\n",
    "    print(f\"Average End Accuracy: {acc_list.mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test task 0: Accuracy: 510/1000 (51.00%)\n",
      "Test task 1: Accuracy: 475/1000 (47.50%)\n",
      "Test task 2: Accuracy: 556/1000 (55.60%)\n",
      "Test task 3: Accuracy: 441/1000 (44.10%)\n",
      "Test task 4: Accuracy: 517/1000 (51.70%)\n",
      "Test task 5: Accuracy: 561/1000 (56.10%)\n",
      "Test task 6: Accuracy: 520/1000 (52.00%)\n",
      "Test task 7: Accuracy: 538/1000 (53.80%)\n",
      "Test task 8: Accuracy: 601/1000 (60.10%)\n",
      "Test task 9: Accuracy: 623/1000 (62.30%)\n",
      "Average End Accuracy: 53.42%\n"
     ]
    }
   ],
   "source": [
    "# test ncm classifier\n",
    "seen_classes = list(set(agent.buffer.y_int.tolist()))\n",
    "agent.model.eval()\n",
    "with torch.no_grad():\n",
    "    acc_list = np.zeros(len(task_loader))\n",
    "    for j in range(10):\n",
    "        correct = torch.full([], 0).cuda()\n",
    "        num = torch.full([], 0).cuda()\n",
    "        for batch_idx, (data, target) in enumerate(task_loader[j]['test']):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            features_ls = agent.model.features(data)\n",
    "            dists_ls = []\n",
    "            for feat_id in range(4):\n",
    "                class_means = class_means_ls[feat_id]\n",
    "                features = features_ls[feat_id]\n",
    "                features = F.normalize(features, dim=1)\n",
    "                features = features.unsqueeze(2)\n",
    "                means = torch.stack([class_means[cls] for cls in seen_classes])\n",
    "                means = torch.stack([means] * data.size(0))\n",
    "                means = means.transpose(1, 2)\n",
    "                features = features.expand_as(means)\n",
    "                dists = (features - means).pow(2).sum(1).squeeze()\n",
    "                dists_ls.append(dists)\n",
    "\n",
    "            dists_ls = torch.cat([dists.unsqueeze(1) for dists in dists_ls], dim=1)\n",
    "            dists = dists_ls.mean(dim=1).squeeze(1)\n",
    "            pred = dists.min(1)[1]\n",
    "            pred = torch.Tensor(seen_classes)[pred].to(data.device)\n",
    "            num += data.size()[0]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "        test_accuracy = (100. * correct / num)\n",
    "        acc_list[j] = test_accuracy\n",
    "        print('Test task {}: Accuracy: {}/{} ({:.2f}%)'.format(j, correct, num, test_accuracy))\n",
    "print(f\"Average End Accuracy: {acc_list.mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test classifier output\n",
    "agent.model.eval()\n",
    "# for feat_id in range(4):\n",
    "with torch.no_grad():\n",
    "    acc_list = np.zeros(len(task_loader))\n",
    "    for j in range(9 + 1):\n",
    "        correct = torch.full([], 0).cuda()\n",
    "        num = torch.full([], 0).cuda()\n",
    "        for batch_idx, (data, target) in enumerate(task_loader[j]['test']):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            # pred = agent.model(data)[feat_id]\n",
    "            pred = agent.model(data)\n",
    "            pred = torch.stack(pred, dim=1)\n",
    "            pred = pred.mean(dim=1).squeeze()\n",
    "            Pred = pred.data.max(1, keepdim=True)[1]\n",
    "            num += data.size()[0]\n",
    "            correct += Pred.eq(target.data.view_as(Pred)).sum()\n",
    "\n",
    "        test_accuracy = (100. * correct / num)\n",
    "        print('Test task {}: Accuracy: {}/{} ({:.2f}%)'.format(j, correct, num, test_accuracy))\n",
    "\n",
    "        acc_list[j] = test_accuracy.item()\n",
    "\n",
    "    print(f\"tasks acc:{acc_list}\")\n",
    "    print(f\"tasks avg acc:{acc_list[:9+1].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSA ce loss modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7230)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(128, 100)\n",
    "y = torch.randn(128, 100)\n",
    "\n",
    "loss_func = torch.nn.SmoothL1Loss(reduction='mean', beta=1)\n",
    "print(loss_func(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(160.2610)\n"
     ]
    }
   ],
   "source": [
    "print(torch.dist(x, y, p=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0065)\n"
     ]
    }
   ],
   "source": [
    "mse = torch.nn.MSELoss(reduction='mean')\n",
    "print(mse(x,y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
